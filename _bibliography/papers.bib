---
---

@ARTICLE{Alfano2024-lh,
  title        = {{Now you see me, now you don’t: an exploration of religious
                  exnomination in DALL-E}},
  author       = {Alfano, Mark and Abedin, Ehsan and Reimann, Ritsaart and
                  Ferreira, Marinus and Cheong, Marc},
  journaltitle = {Ethics and information technology},
  volume       = {26},
  issue        = {2},
  pages        = {27},
  year         = {2024},
  doi          = {10.1007/s10676-024-09760-y},
  issn         = {1388-1957,1572-8439},
  abstract     = {Artificial intelligence (AI) systems are increasingly being
                  used not only to classify and analyze but also to generate
                  images and text. As recent work on the content produced by
                  text and image Generative AIs has shown (e.g., Cheong et al.,
                  2024, Acerbi \& Stubbersfield, 2023), there is a risk that
                  harms of representation and bias, already documented in prior
                  AI and natural language processing (NLP) algorithms may also
                  be present in generative models. These harms relate to
                  protected categories such as gender, race, age, and religion.
                  There are several kinds of harms of representation to consider
                  in this context, including stereotyping, lack of recognition,
                  denigration, under-representation, and many others (Crawford
                  in Soundings 41:45–55, 2009; in: Barocas et al., SIGCIS
                  Conference, 2017). Whereas the bulk of researchers’ attention
                  thus far has been given to stereotyping and denigration, in
                  this study we examine ‘exnomination’, as conceived by Roland
                  Barthes (1972), of religious groups. Our case study is DALL-E,
                  a tool that generates images from natural language prompts.
                  Using DALL-E mini, we generate images from generic prompts
                  such as “religious person.” We then examine whether the
                  generated images are recognizably members of a nominated
                  group. Thus, we assess whether the generated images normalize
                  some religions while neglecting others. We hypothesize that
                  Christianity will be recognizably represented more frequently
                  than other religious groups. Our results partially support
                  this hypothesis but introduce further complexities, which we
                  then explore.},
  html          = {https://doi.org/10.1007/s10676-024-09760-y}
}


@ARTICLE{Alfano2024-hn,
  title        = {{Moral universals: A machine-reading analysis of 256
                  societies}},
  author       = {Alfano, Mark and Cheong, Marc and Curry, Oliver Scott},
  journaltitle = {Heliyon},
  volume       = {10},
  issue        = {6},
  pages        = {e25940},
  year         = {2024},
  doi          = {10.1016/j.heliyon.2024.e25940},
  abstract     = {What is the cross-cultural prevalence of the seven moral
                  values posited by the theory of "morality-as-cooperation"?
                  Previous research, using laborious hand-coding of ethnographic
                  accounts of ethics from 60 societies, found examples of most
                  of the seven morals in most societies, and observed these
                  morals with equal frequency across cultural regions. Here we
                  replicate and extend this analysis by developing a new
                  Morality-as-Cooperation Dictionary (MAC-D) and using
                  Linguistic Inquiry and Word Count (LIWC) to machine-code
                  ethnographic accounts of morality from an additional 196
                  societies (the entire Human Relations Area Files, or HRAF,
                  corpus). Again, we find evidence of most of the seven morals
                  in most societies, across all cultural regions. The new method
                  allows us to detect minor variations in morals across region
                  and subsistence strategy. And we successfully validate the new
                  machine-coding against the previous hand-coding. In light of
                  these findings, MAC-D emerges as a theoretically-motivated,
                  comprehensive, and validated tool for machine-reading moral
                  corpora. We conclude by discussing the limitations of the
                  current study, as well as prospects for future research.},
  html          = {http://dx.doi.org/10.1016/j.heliyon.2024.e25940},
  keywords     = {Cooperation; Ethnography; LIWC; Morality; Natural language
                  processing; Universals},
  language     = {en}
}

@ARTICLE{Cheong2024-ug,
  title        = {{Investigating gender and racial biases in DALL-E Mini Images}},
  author       = {Cheong, Marc and Abedin, Ehsan and Ferreira, Marinus and
                  Reimann, Ritsaart Willem and Chalson, Shalom and Robinson,
                  Pamela and Byrne, Joanne and Ruppanner, Leah and Alfano, Mark
                  and Klein, Colin},
  journaltitle = {ACM J. Responsib. Comput.},
  volume       = {Just Accepted},
  year         = {2024},
  doi          = {10.1145/3649883},
  abstract = {Generative artificial intelligence systems based on transformers, including both text generators such as GPT-4 and image generators such as DALL-E 3, have recently entered the popular consciousness. These tools, while impressive, are liable to reproduce, exacerbate, and reinforce extant human social biases, such as gender and racial biases. In this article, we systematically review the extent to which DALL-E Mini suffers from this problem. In line with the Model Card published alongside DALL-E Mini by its creators, we find that the images it produces tend to represent dozens of different occupations as populated either solely by men (e.g., pilot, builder, plumber) or solely by women (e.g., hairdresser, receptionist, dietitian). In addition, the images DALL-E Mini produces tend to represent most occupations as populated primarily or solely by White people (e.g., farmer, painter, prison officer, software engineer) and very few by non-White people (e.g., pastor, rapper). These findings suggest that exciting new AI technologies should be critically scrutinized and perhaps regulated before they are unleashed on society.},
  html         = {https://philpapers.org/rec/CHEIGA-2}
}

@ARTICLE{Paltiel2023-af,
  title        = {{Approaches and Models for Teaching Digital Ethics in
                  Information Systems Courses – A Review of the Literature}},
  author       = {Paltiel, Minna and Cheong, Marc and Coghlan, Simon and
                  Lederman, Reeva},
  journaltitle = {Australasian Journal of Information Systems},
  publisher    = {Australian Computer Society},
  volume       = {27},
  year         = {2023},
  doi          = {10.3127/ajis.v27i0.4517},
  issn         = {1326-2238,1326-2238},
  abstract     = {The Australasian Journal of Information Systems is a refereed
                  journal that publishes articles contributing to Information
                  Systems theory and practice.},
  html          = {https://journal.acs.org.au/index.php/ajis/article/view/4517},
  urldate      = {2024-06-14},
  keywords     = {information systems; digital ethics; education; pedagogical
                  theories; moral theories},
  language     = {en}
}

@ARTICLE{Coghlan2023-fl,
  title        = {{To chat or bot to chat: Ethical issues with using chatbots in
                  mental health}},
  author       = {Coghlan, Simon and Leins, Kobi and Sheldrick, Susie and
                  Cheong, Marc and Gooding, Piers and D'Alfonso, Simon},
  journaltitle = {Digital health},
  volume       = {9},
  pages        = {20552076231183542},
  year         = {2023},
  doi          = {10.1177/20552076231183542},
  pmc          = {PMC10291862},
  pmid         = {37377565},
  issn         = {2055-2076},
  abstract     = {This paper presents a critical review of key ethical issues
                  raised by the emergence of mental health chatbots. Chatbots
                  use varying degrees of artificial intelligence and are
                  increasingly deployed in many different domains including
                  mental health. The technology may sometimes be beneficial,
                  such as when it promotes access to mental health information
                  and services. Yet, chatbots raise a variety of ethical
                  concerns that are often magnified in people experiencing
                  mental ill-health. These ethical challenges need to be
                  appreciated and addressed throughout the technology pipeline.
                  After identifying and examining four important ethical issues
                  by means of a recognised ethical framework comprised of five
                  key principles, the paper offers recommendations to guide
                  chatbot designers, purveyers, researchers and mental health
                  practitioners in the ethical creation and deployment of
                  chatbots for mental health.},
  html          = {http://dx.doi.org/10.1177/20552076231183542},
  keywords     = {artificial intelligence; chatbots; data privacy; ethics;
                  mental health},
  language     = {en}
}

@ARTICLE{Cheong2023-gq,
  title        = {{Existentialism on social media}},
  author       = {Cheong, Marc},
  journaltitle = {Journal of Human-Technology Relations},
  publisher    = {TU Delft OPEN Publishing},
  volume       = {1},
  year         = {2023},
  doi          = {10.59490/jhtr.2023.1.7022},
  issn         = {2773-2266},
  abstract     = {Social media has become a basis for helping us maintain human
                  contact, especially as our alienation from our
                  phenomenological experiences of ‘being human’ is becoming
                  apparent due to the pandemic. I argue for how existentialist
                  philosophy is crucial, more than ever, to interrogate our
                  social media usage, which is a ‘necessary evil’ in our daily
                  lives. Firstly, Kierkegaard’s critiques of the crowd and of
                  the press are equally applicable to social media, which plays
                  both roles: enabling an anonymous mass of public opinion and
                  doubling-up as an information source, reducing responsibility
                  on the individual. Secondly, social media leads to an
                  intrinsic pressure to objectify one's self (as portrayed) due
                  to the possibility of an omnipresent Other, based on the
                  technological design of networks. I will link my arguments on
                  social media to other existential ideals and will conclude by
                  suggesting changes that may promote existential ideals in
                  one’s social media portrayal and engagement.},
  html          = {https://journals.open.tudelft.nl/jhtr/article/view/7022}
}

@INPROCEEDINGS{Cohney2023-ib,
  title     = {{COVID Down Under: where did Australia's pandemic apps go wrong?}},
  author    = {Cohney, Shaanan and Cheong, Marc},
  booktitle = {{2023 IEEE International Symposium on Ethics in Engineering,
               Science, and Technology (ETHICS)}},
  pages     = {1--8},
  year      = {2023},
  doi       = {10.1109/ETHICS57328.2023.10154912},
  abstract  = {Governments and businesses worldwide deployed a variety of
               technological measures to help prevent and track the spread of
               COVID-19. In Australia, these applications contained usability,
               accessibility, and security flaws that hindered their
               effectiveness and adoption. Australia, like most countries, has
               transitioned to treating COVID as endemic. However it is yet to
               absorb lessons from the technological issues with its approach to
               the pandemic. In this short paper we a) provide a systematization
               of the most notable events; b) identify and review different
               failure modes of these applications; and c) develop
               recommendations for developing apps in the face of future crises.
               Our work focuses on a single country. However, Australia's issues
               are particularly instructive as they highlight surprisingly
               pitfalls that countries should address in the face of a future
               pandemic.},
  html       = {http://dx.doi.org/10.1109/ETHICS57328.2023.10154912},
  keywords  = {
               COVID-19;Ethics;Pandemics;Government;Australia;Security;Usability;COVID-19;QR
               codes;contact tracing;Australia;security;uptake}
}

@INPROCEEDINGS{Creely2023-jv,
  title     = {{ETHICS-2023 Session D1 - Open Forum: Building a Technology
               Ethics Community}},
  author    = {Creely, Thomas and Cheong, Marc and Love, Heather and Schmitt,
               Ketra},
  booktitle = {{2023 IEEE International Symposium on Ethics in Engineering,
               Science, and Technology (ETHICS)}},
  pages     = {01--03},
  year      = {2023},
  doi       = {10.1109/ETHICS57328.2023.10154968},
  abstract  = {This session was an open forum where audience members were
               invited to participate in discussions of a number of themes with
               relevance to ethics and technology and to future conferences in
               the IEEE ETHICS series. The discussions took place in small
               groups, with groups reporting back to the full cohort for
               collaborative brainstorming.},
  html       = {http://dx.doi.org/10.1109/ETHICS57328.2023.10154968}
}

@INPROCEEDINGS{McLoughney2023-wi,
  title     = {{‘Emerging proxies’ in information-rich machine learning: a
               threat to fairness?}},
  author    = {McLoughney, Aidan James and Paterson, Jeannie Marie and Cheong,
               Marc and Wirth, Anthony},
  booktitle = {{2023 IEEE International Symposium on Ethics in Engineering,
               Science, and Technology (ETHICS)}},
  pages     = {1--1},
  year      = {2023},
  doi       = {10.1109/ETHICS57328.2023.10155045},
  abstract  = {Anti-discrimination law in many jurisdictions effectively bans
               the use of race and gender in automated decision-making. For
               example, this law means that insurance companies should not
               explicitly ask about legally protected attributes, e.g., race, in
               order to tailor their premiums to particular customers. In legal
               terms, indirect discrimination occurs when a generally neutral
               rule or variable is used, but significantly negatively affects
               one demographic group. An emerging example of this concern is
               inclusion of proxy variables in Machine Learning (ML) models,
               where neutral variables are predictive of protected attributes.
               For example, postcodes or zip codes are representative of
               communities, and therefore racial demographics and
               social-economic class; i.e., a traditional example of ‘redlining’
               pre-dating modern automated techniques [1]. The law struggles
               with proxy variables in machine learning: indirect discrimination
               cases are difficult to bring to court, particularly because
               finding substantial evidence that shows the indirect
               discrimination to be unlawful is difficult [2]. With more complex
               machine-learning models being developed for automated decision
               making, e.g., random forests or state-of-the-art deep neural
               networks, more data points on customers are accumulated [1], from
               a wide variety of sources. With such rich data, ML models can
               produce multiple interconnected correlations - such as that found
               in single neurons in a neural network, or single decision trees
               in a random forest - which are predictive of protected
               attributes, akin to traditional uses of discrete proxy variables.
               In this poster, we introduce the concept of "emerging proxies",
               that are a combination of several variables, from which the ML
               model could infer the protected attribute(s) of the individuals
               in the dataset. This concept differs from the traditional concept
               of proxies because rather than addressing a single proxy
               variable, a distribution of interconnected proxies would have to
               be addressed. Our contribution is to provide evidence for the
               capacity of complex ML models to identify protected attributes
               through the correlation of other variables. This correlation is
               not made explicitly through a discrete one to one relationship
               between variables, but through a many-to-one relationship. This
               contribution complements concerns raised in legal analyses of
               automated decision-making about proxies in ML models leading to
               indirect discrimination [3]. Our contribution shows that if an ML
               model contains “emerging proxies” for a protected attribute, the
               distribution of proxies will be a roadblock when attempting to
               de-bias the model, limiting the pathways available for addressing
               potential discrimination caused by the ML model.},
  html       = {http://dx.doi.org/10.1109/ETHICS57328.2023.10155045},
  keywords  = {Law;Ethics;Australia;Decision making;Correlation;Random
               forests;Predictive models;algorithmic
               fairness;proxies;discrimination;consumer law;credit}
}

@ARTICLE{Caddy2023-qm,
  title        = {{"Tell us what's going on": Exploring the information needs of
                  pregnant and post-partum women in Australia during the
                  pandemic with 'Tweets', 'Threads', and women's views}},
  author       = {Caddy, Cassandra and Cheong, Marc and Lim, Megan S C and
                  Power, Robert and Vogel, Joshua P and Bradfield, Zoe and
                  Coghlan, Benjamin and Homer, Caroline S E and Wilson, Alyce N},
  journaltitle = {PloS one},
  volume       = {18},
  issue        = {1},
  pages        = {e0279990},
  year         = {2023},
  doi          = {10.1371/journal.pone.0279990},
  pmc          = {PMC9838848},
  pmid         = {36638130},
  issn         = {1932-6203},
  abstract     = {INTRODUCTION: The provision of maternity services in Australia
                  has been significantly disrupted in response to the COVID-19
                  pandemic. Many changes were initiated quickly, often with
                  rapid dissemination of information to women. The aim of this
                  study was to better understand what information and messages
                  were circulating regarding COVID-19 and pregnancy in Australia
                  and potential information gaps. METHODS: This study adopted a
                  qualitative approach using social media and interviews. A data
                  analytics tool (TIGER-C19) was used to extract data from
                  social media platforms Reddit and Twitter from June to July
                  2021 (in the middle of the third COVID-19 wave in Australia).
                  A total of 21 individual semi-structured interviews were
                  conducted with those who were, or had been, pregnant in
                  Australia since March 2020. Social media data were analysis
                  via inductive content analysis and interview data were
                  thematically analysed. RESULTS: Social media provided a
                  critical platform for sharing and seeking information, as well
                  as highlighting attitudes of the community towards COVID-19
                  vaccines in pregnancy. Women interviewed described wanting
                  further information on the risks COVID-19 posed to themselves
                  and their babies, and greater familiarity with the health
                  service during pregnancy, in which they would labour and give
                  birth. Health providers were a trusted source of information.
                  Communication strategies that allowed participants to engage
                  in real-time interactive discussions were preferred. A real or
                  perceived lack of information led participants to turn to
                  informal sources, increasing the potential for exposure to
                  misinformation. CONCLUSION: It is vital that health services
                  communicate effectively with pregnant women, early and often
                  throughout public health crises, such as the COVID-19
                  pandemic. This was particularly important during periods of
                  increased restrictions on accessing hospital services.
                  Information and communication strategies need to be clear,
                  consistent, timely and accessible to reduce reliance on
                  informal and potentially inaccurate sources.},
  html          = {http://dx.doi.org/10.1371/journal.pone.0279990},
  language     = {en}
}

@INPROCEEDINGS{Klein2023-oy,
  title     = {{The wisdom\_of\_crowds: An Efficient, Philosophically-Validated,
               Social Epistemological Network Profiling Toolkit}},
  author    = {Klein, Colin and Cheong, Marc and Ferreira, Marinus and Sullivan,
               Emily and Alfano, Mark},
  booktitle = {{Complex Networks and Their Applications XI}},
  publisher = {Springer International Publishing},
  pages     = {62--73},
  year      = {2023},
  doi       = {10.1007/978-3-031-21127-0_6},
  abstract  = {The epistemic position of an agent often depends on their
               position in a larger network of other agents who provide them
               with information. In general, agents are better off if they have
               diverse and independent sources. Sullivan et al. [19] developed a
               method for quantitatively characterizing the epistemic position
               of individuals in a network that takes into account both
               diversity and independence and presented a proof-of-concept,
               closed-source implementation on a small graph derived from
               Twitter data [19]. This paper reports on an open-source
               re-implementation of their algorithm in Python, optimized to be
               usable on much larger networks. In addition to the algorithm and
               package, we also show the ability to scale up our package to
               large synthetic social network graph profiling, and finally
               demonstrate its utility in analyzing real-world empirical
               evidence of ‘echo chambers’ on online social media, as well as
               interdisciplinary diversity in an academic communications
               network.},
  html       = {http://dx.doi.org/10.1007/978-3-031-21127-0_6}
}

@ARTICLE{Abedin2023-wa,
  title        = {{Exploring intellectual humility through the lens of
                  artificial intelligence: Top terms, features and a predictive
                  model}},
  author       = {Abedin, Ehsan and Ferreira, Marinus and Reimann, Ritsaart and
                  Cheong, Marc and Grossmann, Igor and Alfano, Mark},
  journaltitle = {Acta psychologica},
  volume       = {238},
  pages        = {103979},
  year         = {2023},
  doi          = {10.1016/j.actpsy.2023.103979},
  issn         = {0001-6918},
  abstract     = {Intellectual humility (IH) is often conceived as the
                  recognition of, and appropriate response to, your own
                  intellectual limitations. As far as we are aware, only a
                  handful of studies look at interventions to increase IH – e.g.
                  through journalling – and no study so far explores the extent
                  to which having high or low IH can be predicted. This paper
                  uses machine learning and natural language processing
                  techniques to develop a predictive model for IH and identify
                  top terms and features that indicate degrees of IH. We trained
                  our classifier on the dataset from an existing psychological
                  study on IH, where participants were asked to journal their
                  experiences with handling social conflicts over 30 days. We
                  used Logistic Regression (LR) to train a classifier and the
                  Linguistic Inquiry and Word Count (LIWC) dictionaries for
                  feature selection, picking out a range of word categories
                  relevant to interpersonal relationships. Our results show that
                  people who differ on IH do in fact systematically express
                  themselves in different ways, including through expression of
                  emotions (i.e., positive, negative, and specifically anger,
                  anxiety, sadness, as well as the use of swear words), use of
                  pronouns (i.e., first person, second person, and third person)
                  and time orientation (i.e., past, present, and future tenses).
                  We discuss the importance of these findings for IH and the
                  value of using such techniques for similar psychological
                  studies, as well as some ethical concerns and limitations with
                  the use of such semi-automated classifications.},
  html          = {https://www.sciencedirect.com/science/article/pii/S0001691823001555},
  keywords     = {Intellectual humility, Artificial intelligence, Natural
                  language processing, Social conflicts, Daily journalling}
}

@INPROCEEDINGS{Cheong2023-ht,
  title       = {{Generative AIs: Epistemic injustice in AI, writ large?}},
  author      = {Cheong, Marc and Abedin, E},
  booktitle   = {{2023 Australasian Association of Philosophy Conference}},
  institution = {Australian Catholic University},
  location    = {Melbourne, VIC.},
  venue       = {Australian Catholic University, Melbourne, VIC, Australia},
  year        = {2023},
  abstract    = {Fricker's (2007) framework on epistemic injustice has been
                 applied to Artificial Intelligence (AI) systems, to explain the
                 harm to epistemic agents who use and interact with such
                 systems. These AI systems range from chatbots; to algorithmic
                 decision-makers such as automated hiring systems; to social
                 media recommenders. The rapid release, deployment, and use of
                 Generative AI systems (GAIs) -- fro text generators (e.g.,
                 ChatGPT) to image generators (e.g., DALL-E 2) -- are a case in
                 point. Users of GAIs running the risk of harm as knowers, based
                 on the credibility excess afforded to the GAI, despite their
                 propensity to 'hallucinate' or make up facts. GAIs also inflict
                 hermeneutical injustice, as their data models are not truly
                 representative of society at large: an example is the issue of
                 racial and gender biases in GAIs. Using empirical evidence from
                 the modern crop of GAIs, this talk will illustrate the extent
                 of epistemic injustice in GAIs, drawing upon Fricker (2007) as
                 a framework. We will especially focus on how these injustices
                 can be entrenched in such GAIs, owing to the way the data
                 models are "updated" from version to version, and the ability
                 for these GAIs to be 'cross-pollinated' with other AI systems.},
  keywords    = {Social epistemology, Fricker}
}

@REPORT{Ruppanner2023-vi,
  type        = {resreport},
  title       = {{2023 State of the future of work}},
  author      = {Ruppanner, Leah and Churchill, Brendan and Bissell, David and
                 Ghin, Peter and Hydelund, Camilla and Ainsworth, Susan and
                 Blackham, Alysia and Borland, Jeff and Cheong, Marc and Evans,
                 Michelle and Frermann, Lea and King, Tania and Vetere, Frank},
  publisher   = {University of Melbourne},
  institution = {The University of Melbourne},
  year        = {2023},
  pdf         = {https://apo.org.au/sites/default/files/resource-files/2023-03/apo-nid322034.pdf}
}



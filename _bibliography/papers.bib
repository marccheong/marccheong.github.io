---
---

@ARTICLE{Alfano2024-lh,
  title        = {{Now you see me, now you don’t: an exploration of religious
                  exnomination in DALL-E}},
  author       = {Alfano, Mark and Abedin, Ehsan and Reimann, Ritsaart and
                  Ferreira, Marinus and Cheong, Marc},
  journaltitle = {Ethics and information technology},
  volume       = {26},
  issue        = {2},
  pages        = {27},
  year         = {2024},
  doi          = {10.1007/s10676-024-09760-y},
  issn         = {1388-1957,1572-8439},
  abstract     = {Artificial intelligence (AI) systems are increasingly being
                  used not only to classify and analyze but also to generate
                  images and text. As recent work on the content produced by
                  text and image Generative AIs has shown (e.g., Cheong et al.,
                  2024, Acerbi \& Stubbersfield, 2023), there is a risk that
                  harms of representation and bias, already documented in prior
                  AI and natural language processing (NLP) algorithms may also
                  be present in generative models. These harms relate to
                  protected categories such as gender, race, age, and religion.
                  There are several kinds of harms of representation to consider
                  in this context, including stereotyping, lack of recognition,
                  denigration, under-representation, and many others (Crawford
                  in Soundings 41:45–55, 2009; in: Barocas et al., SIGCIS
                  Conference, 2017). Whereas the bulk of researchers’ attention
                  thus far has been given to stereotyping and denigration, in
                  this study we examine ‘exnomination’, as conceived by Roland
                  Barthes (1972), of religious groups. Our case study is DALL-E,
                  a tool that generates images from natural language prompts.
                  Using DALL-E mini, we generate images from generic prompts
                  such as “religious person.” We then examine whether the
                  generated images are recognizably members of a nominated
                  group. Thus, we assess whether the generated images normalize
                  some religions while neglecting others. We hypothesize that
                  Christianity will be recognizably represented more frequently
                  than other religious groups. Our results partially support
                  this hypothesis but introduce further complexities, which we
                  then explore.},
  html          = {https://doi.org/10.1007/s10676-024-09760-y}
}


@ARTICLE{Alfano2024-hn,
  title        = {{Moral universals: A machine-reading analysis of 256
                  societies}},
  author       = {Alfano, Mark and Cheong, Marc and Curry, Oliver Scott},
  journaltitle = {Heliyon},
  volume       = {10},
  issue        = {6},
  pages        = {e25940},
  year         = {2024},
  doi          = {10.1016/j.heliyon.2024.e25940},
  abstract     = {What is the cross-cultural prevalence of the seven moral
                  values posited by the theory of "morality-as-cooperation"?
                  Previous research, using laborious hand-coding of ethnographic
                  accounts of ethics from 60 societies, found examples of most
                  of the seven morals in most societies, and observed these
                  morals with equal frequency across cultural regions. Here we
                  replicate and extend this analysis by developing a new
                  Morality-as-Cooperation Dictionary (MAC-D) and using
                  Linguistic Inquiry and Word Count (LIWC) to machine-code
                  ethnographic accounts of morality from an additional 196
                  societies (the entire Human Relations Area Files, or HRAF,
                  corpus). Again, we find evidence of most of the seven morals
                  in most societies, across all cultural regions. The new method
                  allows us to detect minor variations in morals across region
                  and subsistence strategy. And we successfully validate the new
                  machine-coding against the previous hand-coding. In light of
                  these findings, MAC-D emerges as a theoretically-motivated,
                  comprehensive, and validated tool for machine-reading moral
                  corpora. We conclude by discussing the limitations of the
                  current study, as well as prospects for future research.},
  html          = {http://dx.doi.org/10.1016/j.heliyon.2024.e25940},
  keywords     = {Cooperation; Ethnography; LIWC; Morality; Natural language
                  processing; Universals},
  language     = {en}
}

@ARTICLE{Cheong2024-ug,
  title        = {{Investigating gender and racial biases in DALL-E Mini Images}},
  author       = {Cheong, Marc and Abedin, Ehsan and Ferreira, Marinus and
                  Reimann, Ritsaart Willem and Chalson, Shalom and Robinson,
                  Pamela and Byrne, Joanne and Ruppanner, Leah and Alfano, Mark
                  and Klein, Colin},
  journaltitle = {ACM J. Responsib. Comput.},
  volume       = {Just Accepted},
  year         = {2024},
  doi          = {10.1145/3649883},
  abstract = {Generative artificial intelligence systems based on transformers, including both text generators such as GPT-4 and image generators such as DALL-E 3, have recently entered the popular consciousness. These tools, while impressive, are liable to reproduce, exacerbate, and reinforce extant human social biases, such as gender and racial biases. In this article, we systematically review the extent to which DALL-E Mini suffers from this problem. In line with the Model Card published alongside DALL-E Mini by its creators, we find that the images it produces tend to represent dozens of different occupations as populated either solely by men (e.g., pilot, builder, plumber) or solely by women (e.g., hairdresser, receptionist, dietitian). In addition, the images DALL-E Mini produces tend to represent most occupations as populated primarily or solely by White people (e.g., farmer, painter, prison officer, software engineer) and very few by non-White people (e.g., pastor, rapper). These findings suggest that exciting new AI technologies should be critically scrutinized and perhaps regulated before they are unleashed on society.},
  html         = {https://philpapers.org/rec/CHEIGA-2}
}

@ARTICLE{Paltiel2023-af,
  title        = {{Approaches and Models for Teaching Digital Ethics in
                  Information Systems Courses – A Review of the Literature}},
  author       = {Paltiel, Minna and Cheong, Marc and Coghlan, Simon and
                  Lederman, Reeva},
  journaltitle = {Australasian Journal of Information Systems},
  publisher    = {Australian Computer Society},
  volume       = {27},
  year         = {2023},
  doi          = {10.3127/ajis.v27i0.4517},
  issn         = {1326-2238,1326-2238},
  abstract     = {The Australasian Journal of Information Systems is a refereed
                  journal that publishes articles contributing to Information
                  Systems theory and practice.},
  html          = {https://journal.acs.org.au/index.php/ajis/article/view/4517},
  urldate      = {2024-06-14},
  keywords     = {information systems; digital ethics; education; pedagogical
                  theories; moral theories},
  language     = {en}
}


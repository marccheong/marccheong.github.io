---
---

@ARTICLE{Alfano2024-lh,
  title        = {{Now you see me, now you don’t: an exploration of religious
                  exnomination in DALL-E}},
  author       = {Alfano, Mark and Abedin, Ehsan and Reimann, Ritsaart and
                  Ferreira, Marinus and Cheong, Marc},
  journaltitle = {Ethics and information technology},
  volume       = {26},
  issue        = {2},
  pages        = {27},
  date         = {2024-04-12},
  doi          = {10.1007/s10676-024-09760-y},
  issn         = {1388-1957,1572-8439},
  abstract     = {Artificial intelligence (AI) systems are increasingly being
                  used not only to classify and analyze but also to generate
                  images and text. As recent work on the content produced by
                  text and image Generative AIs has shown (e.g., Cheong et al.,
                  2024, Acerbi \& Stubbersfield, 2023), there is a risk that
                  harms of representation and bias, already documented in prior
                  AI and natural language processing (NLP) algorithms may also
                  be present in generative models. These harms relate to
                  protected categories such as gender, race, age, and religion.
                  There are several kinds of harms of representation to consider
                  in this context, including stereotyping, lack of recognition,
                  denigration, under-representation, and many others (Crawford
                  in Soundings 41:45–55, 2009; in: Barocas et al., SIGCIS
                  Conference, 2017). Whereas the bulk of researchers’ attention
                  thus far has been given to stereotyping and denigration, in
                  this study we examine ‘exnomination’, as conceived by Roland
                  Barthes (1972), of religious groups. Our case study is DALL-E,
                  a tool that generates images from natural language prompts.
                  Using DALL-E mini, we generate images from generic prompts
                  such as “religious person.” We then examine whether the
                  generated images are recognizably members of a nominated
                  group. Thus, we assess whether the generated images normalize
                  some religions while neglecting others. We hypothesize that
                  Christianity will be recognizably represented more frequently
                  than other religious groups. Our results partially support
                  this hypothesis but introduce further complexities, which we
                  then explore.},
  url          = {https://doi.org/10.1007/s10676-024-09760-y}
}

@ARTICLE{Alfano2024-hn,
  title        = {{Moral universals: A machine-reading analysis of 256
                  societies}},
  author       = {Alfano, Mark and Cheong, Marc and Curry, Oliver Scott},
  journaltitle = {Heliyon},
  volume       = {10},
  issue        = {6},
  pages        = {e25940},
  date         = {2024-03-30},
  doi          = {10.1016/j.heliyon.2024.e25940},
  pmc          = {PMC10945118},
  pmid         = {38501007},
  issn         = {2405-8440},
  abstract     = {What is the cross-cultural prevalence of the seven moral
                  values posited by the theory of "morality-as-cooperation"?
                  Previous research, using laborious hand-coding of ethnographic
                  accounts of ethics from 60 societies, found examples of most
                  of the seven morals in most societies, and observed these
                  morals with equal frequency across cultural regions. Here we
                  replicate and extend this analysis by developing a new
                  Morality-as-Cooperation Dictionary (MAC-D) and using
                  Linguistic Inquiry and Word Count (LIWC) to machine-code
                  ethnographic accounts of morality from an additional 196
                  societies (the entire Human Relations Area Files, or HRAF,
                  corpus). Again, we find evidence of most of the seven morals
                  in most societies, across all cultural regions. The new method
                  allows us to detect minor variations in morals across region
                  and subsistence strategy. And we successfully validate the new
                  machine-coding against the previous hand-coding. In light of
                  these findings, MAC-D emerges as a theoretically-motivated,
                  comprehensive, and validated tool for machine-reading moral
                  corpora. We conclude by discussing the limitations of the
                  current study, as well as prospects for future research.},
  url          = {http://dx.doi.org/10.1016/j.heliyon.2024.e25940},
  keywords     = {Cooperation; Ethnography; LIWC; Morality; Natural language
                  processing; Universals},
  language     = {en}
}

@ARTICLE{Cheong2024-ug,
  title        = {{Investigating gender and racial biases in DALL-E Mini Images}},
  author       = {Cheong, Marc and Abedin, Ehsan and Ferreira, Marinus and
                  Reimann, Ritsaart Willem and Chalson, Shalom and Robinson,
                  Pamela and Byrne, Joanne and Ruppanner, Leah and Alfano, Mark
                  and Klein, Colin},
  journaltitle = {ACM J. Responsib. Comput.},
  volume       = {Just Accepted},
  date         = {2024},
  doi          = {10.1145/3649883},
  url          = {https://philpapers.org/rec/CHEIGA-2}
}

@INPROCEEDINGS{Cheong2023-ht,
  title       = {{Generative AIs: Epistemic injustice in AI, writ large?}},
  author      = {Cheong, Marc and Abedin, E},
  booktitle   = {{2023 Australasian Association of Philosophy Conference}},
  institution = {Australian Catholic University},
  location    = {Melbourne, VIC.},
  venue       = {Australian Catholic University, Melbourne, VIC, Australia},
  date        = {2023},
  abstract    = {Fricker's (2007) framework on epistemic injustice has been
                 applied to Artificial Intelligence (AI) systems, to explain the
                 harm to epistemic agents who use and interact with such
                 systems. These AI systems range from chatbots; to algorithmic
                 decision-makers such as automated hiring systems; to social
                 media recommenders. The rapid release, deployment, and use of
                 Generative AI systems (GAIs) -- fro text generators (e.g.,
                 ChatGPT) to image generators (e.g., DALL-E 2) -- are a case in
                 point. Users of GAIs running the risk of harm as knowers, based
                 on the credibility excess afforded to the GAI, despite their
                 propensity to 'hallucinate' or make up facts. GAIs also inflict
                 hermeneutical injustice, as their data models are not truly
                 representative of society at large: an example is the issue of
                 racial and gender biases in GAIs. Using empirical evidence from
                 the modern crop of GAIs, this talk will illustrate the extent
                 of epistemic injustice in GAIs, drawing upon Fricker (2007) as
                 a framework. We will especially focus on how these injustices
                 can be entrenched in such GAIs, owing to the way the data
                 models are "updated" from version to version, and the ability
                 for these GAIs to be 'cross-pollinated' with other AI systems.},
  keywords    = {Social epistemology, Fricker}
}

@ARTICLE{Paltiel2023-af,
  title        = {{Approaches and Models for Teaching Digital Ethics in
                  Information Systems Courses – A Review of the Literature}},
  author       = {Paltiel, Minna and Cheong, Marc and Coghlan, Simon and
                  Lederman, Reeva},
  journaltitle = {Australasian Journal of Information Systems},
  publisher    = {Australian Computer Society},
  volume       = {27},
  date         = {2023-12-26},
  doi          = {10.3127/ajis.v27i0.4517},
  issn         = {1326-2238,1326-2238},
  abstract     = {The Australasian Journal of Information Systems is a refereed
                  journal that publishes articles contributing to Information
                  Systems theory and practice.},
  url          = {https://journal.acs.org.au/index.php/ajis/article/view/4517},
  urldate      = {2024-06-14},
  keywords     = {information systems; digital ethics; education; pedagogical
                  theories; moral theories},
  language     = {en}
}


@ARTICLE{Coghlan2023-fl,
  title        = {{To chat or bot to chat: Ethical issues with using chatbots in
                  mental health}},
  author       = {Coghlan, Simon and Leins, Kobi and Sheldrick, Susie and
                  Cheong, Marc and Gooding, Piers and D'Alfonso, Simon},
  journaltitle = {Digital health},
  volume       = {9},
  pages        = {20552076231183542},
  date         = {2023-06-22},
  doi          = {10.1177/20552076231183542},
  pmc          = {PMC10291862},
  pmid         = {37377565},
  issn         = {2055-2076},
  abstract     = {This paper presents a critical review of key ethical issues
                  raised by the emergence of mental health chatbots. Chatbots
                  use varying degrees of artificial intelligence and are
                  increasingly deployed in many different domains including
                  mental health. The technology may sometimes be beneficial,
                  such as when it promotes access to mental health information
                  and services. Yet, chatbots raise a variety of ethical
                  concerns that are often magnified in people experiencing
                  mental ill-health. These ethical challenges need to be
                  appreciated and addressed throughout the technology pipeline.
                  After identifying and examining four important ethical issues
                  by means of a recognised ethical framework comprised of five
                  key principles, the paper offers recommendations to guide
                  chatbot designers, purveyers, researchers and mental health
                  practitioners in the ethical creation and deployment of
                  chatbots for mental health.},
  url          = {http://dx.doi.org/10.1177/20552076231183542},
  keywords     = {artificial intelligence; chatbots; data privacy; ethics;
                  mental health},
  language     = {en}
}

